{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process  # Fuzzy matching for flexibility\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\nick_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Download all necessary NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # NEW - required for POS tagging\n",
    "\n",
    "# Initialize lemmatizer and stopword list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to convert NLTK POS to WordNet POS\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # fallback\n",
    "\n",
    "# Updated preprocess_text function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    words = text.lower().split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    lemmatized_words = []\n",
    "    for word, pos in tagged_words:\n",
    "        wn_pos = get_wordnet_pos(pos)\n",
    "        lemma = lemmatizer.lemmatize(word, wn_pos)\n",
    "        if lemma == word and wn_pos != wordnet.VERB:\n",
    "            # Retry as verb if unchanged\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmatized_words.append(lemma)\n",
    "\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "def remove_bracketed_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove all content inside brackets (including brackets)\n",
    "    cleaned_text = re.sub(r'\\s*\\([^)]*\\)', '', text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Function to find synonyms using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace(\"_\", \" \"))  # Convert underscores to spaces\n",
    "    return list(synonyms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening the ESCO Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"../data/ESCO skill taxonomy dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Columns containing skills\n",
    "skill_columns = [\n",
    "    \"Essential Skills (Skill)\",\n",
    "    \"Essential Skills (Knowledge)\",\n",
    "    \"Optional Skills (Skill)\",\n",
    "    \"Optional Skills (Knowledge)\"\n",
    "]\n",
    "\n",
    "# Ensure missing values are handled\n",
    "df[skill_columns] = df[skill_columns].fillna(\"\")\n",
    "\n",
    "# Split comma-separated skills into lists\n",
    "for col in skill_columns:\n",
    "    df[col] = df[col].apply(lambda x: x.split(\", \") if isinstance(x, str) else [])\n",
    "\n",
    "# Explode each skill column separately and merge back\n",
    "esco_taxonomy_df = df.copy()\n",
    "for col in skill_columns:\n",
    "    esco_taxonomy_df = esco_taxonomy_df.explode(col)\n",
    "\n",
    "# Rename columns for clarity\n",
    "esco_taxonomy_df = esco_taxonomy_df.rename(columns={\n",
    "    \"Occupation Title\": \"Job Role\",\n",
    "    \"ESCO Code\": \"ESCO Code\",\n",
    "    \"Description\": \"Job Description\",\n",
    "    \"Alternative Labels\": \"Alternative Titles\",\n",
    "    \"Essential Skills (Skill)\": \"Essential Skill\",\n",
    "    \"Essential Skills (Knowledge)\": \"Essential Knowledge\",\n",
    "    \"Optional Skills (Skill)\": \"Optional Skill\",\n",
    "    \"Optional Skills (Knowledge)\": \"Optional Knowledge\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_preprocess = [\n",
    "    \"Essential Skill\",\n",
    "    \"Essential Knowledge\",\n",
    "    \"Optional Skill\",\n",
    "    \"Optional Knowledge\"\n",
    "]\n",
    "\n",
    "for col in columns_to_preprocess:\n",
    "    unique_values = esco_taxonomy_df[col].dropna().unique()\n",
    "    processed_map = {val: preprocess_text(val) for val in unique_values}\n",
    "    esco_taxonomy_df[col] = esco_taxonomy_df[col].map(processed_map)\n",
    "\n",
    "esco_taxonomy_df[\"Essential Skill\"] = esco_taxonomy_df[\"Essential Skill\"].apply(remove_bracketed_words)\n",
    "esco_taxonomy_df[\"Essential Knowledge\"] = esco_taxonomy_df[\"Essential Knowledge\"].apply(remove_bracketed_words)\n",
    "esco_taxonomy_df[\"Optional Skill\"] = esco_taxonomy_df[\"Optional Skill\"].apply(remove_bracketed_words)\n",
    "esco_taxonomy_df[\"Optional Knowledge\"] = esco_taxonomy_df[\"Optional Knowledge\"].apply(remove_bracketed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching skills to jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all valid skills for fuzzy matching\n",
    "all_skills = set(\n",
    "    esco_taxonomy_df[\"Essential Skill\"].dropna().tolist() +\n",
    "    esco_taxonomy_df[\"Essential Knowledge\"].dropna().tolist() +\n",
    "    esco_taxonomy_df[\"Optional Skill\"].dropna().tolist() +\n",
    "    esco_taxonomy_df[\"Optional Knowledge\"].dropna().tolist()\n",
    ")\n",
    "\n",
    "essential_skills = set(    \n",
    "    esco_taxonomy_df[\"Essential Skill\"].dropna().tolist() +\n",
    "    esco_taxonomy_df[\"Essential Knowledge\"].dropna().tolist()\n",
    "    )\n",
    "\n",
    "optional_skills = set(    \n",
    "    esco_taxonomy_df[\"Optional Skill\"].dropna().tolist() +\n",
    "    esco_taxonomy_df[\"Optional Knowledge\"].dropna().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/tools_grouped.csv\"\n",
    "\n",
    "# Load Technology Tool Mapping\n",
    "tech_tools_df = pd.read_csv(file_path)  # File containing tool-category mappings\n",
    "tool_to_category = {}\n",
    "\n",
    "for _, row in tech_tools_df.iterrows():\n",
    "    raw_category = row[\"Technology Tool\"]\n",
    "    raw_tools = row[\"Technology Tool Example\"].split(\", \")\n",
    "\n",
    "    # Preprocess the category name once\n",
    "    category = preprocess_text(raw_category)\n",
    "\n",
    "    for tool in raw_tools:\n",
    "        preprocessed_tool = preprocess_text(tool)\n",
    "        tool_to_category[preprocessed_tool] = category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs intelligent matching between user-entered skills and job roles in the ESCO taxonomy, using a combination of fuzzy string matching and tool-category mapping. The map_skills_to_matched_tools() function takes an individual skill and tries to match it either directly to a known tool name or to a broader tool category using fuzzy matching (with a configurable similarity threshold). It returns the best-matching tool category based on score comparisons. The main function, match_jobs(), processes a list of user-input skills by first cleaning them and then attempting to match each one — prioritizing exact matches for short single-character skills (like \"R\"), followed by fuzzy matches against tools and the ESCO skills dataset. Top matches are selected based on similarity scores, and low-confidence or ambiguous matches are filtered out. Once the matching is done, the function aggregates all matched skills and compares them against job roles in the ESCO taxonomy by constructing a set of all skills (essential and optional) associated with each job. It counts how many matched user skills appear in each job's skill set, filters out roles with fewer than two matches, and returns a ranked list of job roles based on how well they align with the user’s skills. The output includes the number of overlapping skills per job and displays the best-matching roles for easier interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_skills_to_matched_tools(skill, threshold=85):\n",
    "    \"\"\"\n",
    "    Matches user-entered skill to either a tool or tool category based on fuzzy score.\n",
    "    Returns the best-matched tool category.\n",
    "    \"\"\"\n",
    "    tool_names = list(tool_to_category.keys())\n",
    "    tool_categories = list(set(tool_to_category.values()))\n",
    "    skill_lower = preprocess_text(skill)\n",
    "\n",
    "    # Fuzzy match with tool names\n",
    "    match_result_tool = process.extractOne(skill_lower, tool_names, scorer=fuzz.WRatio)\n",
    "    tool_match, tool_score = match_result_tool[:2] if match_result_tool else (None, 0)\n",
    "    tool_category = tool_to_category.get(tool_match) if tool_match else None\n",
    "\n",
    "    # Fuzzy match with category names\n",
    "    match_result_category = process.extractOne(skill_lower, tool_categories, scorer=fuzz.WRatio)\n",
    "    category_match, category_score = match_result_category[:2] if match_result_category else (None, 0)\n",
    "\n",
    "    # Decide based on best score\n",
    "    if tool_score >= threshold and (tool_score > category_score):\n",
    "        print(f\"🔁 '{skill}' matched to tool '{tool_match}' → category '{tool_category}' (score: {tool_score})\")\n",
    "        return tool_category\n",
    "    elif category_score >= threshold:\n",
    "        print(f\"🔁 '{skill}' matched directly to category '{category_match}' (score: {category_score})\")\n",
    "        return category_match\n",
    "\n",
    "    # No good match\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Main Matching Function\n",
    "def match_jobs(user_skills, threshold): \n",
    "    \"\"\"\n",
    "    Matches user-entered skills with job roles using NLP + fuzzy matching.\n",
    "    Filters jobs that have at least 2 matched skills.\n",
    "    \"\"\"\n",
    "\n",
    "    matched_skills = set()\n",
    "\n",
    "    # Preprocess user input skills\n",
    "    user_skills = [preprocess_text(skill) for skill in user_skills]\n",
    "\n",
    "    # Ensure all_skills only contains valid skills\n",
    "    cleaned_skills = set(skill for skill in all_skills if isinstance(skill, str) and skill.strip())\n",
    "\n",
    "    # Perform fuzzy matching for each user skill\n",
    "    for skill in user_skills:\n",
    "        print(f\"\\n🔍 Matching User Skill: {skill}\")\n",
    "\n",
    "        # ✅ Exact match logic for single-character user skills (e.g., \"r\")\n",
    "        if len(skill.strip()) == 1:\n",
    "            exact_matches = [s for s in cleaned_skills if s.strip().lower() == skill.strip().lower()]\n",
    "            if exact_matches:\n",
    "                matched_skills.add(exact_matches[0])\n",
    "                print(f\"✅ Exact Match for Short Skill: {exact_matches[0]}\")\n",
    "            else:\n",
    "                print(\"❌ No exact match found for short skill.\")\n",
    "            continue  # Skip fuzzy matching for this skill\n",
    "\n",
    "        # Tool match\n",
    "        tool_raw = map_skills_to_matched_tools(skill, threshold=threshold)\n",
    "        tool_match = preprocess_text(tool_raw) if tool_raw else None\n",
    "        tool_match_result = process.extractOne(tool_match, cleaned_skills, scorer=fuzz.WRatio) if tool_match else None\n",
    "        tool_best, tool_score = tool_match_result[:2] if tool_match_result else (None, 0)\n",
    "        print(f\"Found Tool Match (Tech Tool): {tool_best} with score: {tool_score}\")\n",
    "        # Fuzzy match top 5\n",
    "        top_matches = process.extract(skill, cleaned_skills, scorer=fuzz.WRatio, limit=5)\n",
    "        print(\"Top Matches:\")\n",
    "        for i, (match, score, _) in enumerate(top_matches, start=1):\n",
    "            print(f\"  {i}. {match} ({round(score, 1)})\")\n",
    "\n",
    "        # ✅ Match all top skills with the same max score above threshold\n",
    "        if tool_best and tool_score >= threshold and tool_score > top_matches[0][1]:\n",
    "            print(f\"✅ Selected Tool Match (Tech Tool): {tool_best}\")\n",
    "            matched_skills.add(tool_best)\n",
    "        elif top_matches:\n",
    "            max_score = top_matches[0][1]\n",
    "            if max_score >= threshold:\n",
    "                for match, score, _ in top_matches:\n",
    "                    if score == max_score:\n",
    "                        # ✅ Prevent single-letter *matched* skills unless perfect match\n",
    "                        if len(match.strip()) == 1 and score < 100:\n",
    "                            print(f\"⚠️ Skipped short matched skill '{match}' (score: {score}) — not an exact match.\")\n",
    "                            continue\n",
    "                        matched_skills.add(match)\n",
    "                        print(f\"✅ Selected Tie Match: {match} ({round(score, 1)})\")\n",
    "            elif tool_best and tool_score >= threshold:\n",
    "                matched_skills.add(tool_best)\n",
    "                print(f\"✅ Selected Tool Match (Tech Tool): {tool_best}\")\n",
    "        else:\n",
    "            print(\"❌ No good match found.\")\n",
    "\n",
    "    if not matched_skills:\n",
    "        print(\"\\n🚫 No valid matches found.\")\n",
    "        return pd.DataFrame(columns=[\"Job Role\", \"Matched Skills Count\"])\n",
    "\n",
    "    # Convert matched skills to lowercase for consistency\n",
    "    matched_skills = set(skill.lower().strip() for skill in matched_skills)\n",
    "\n",
    "    # Step 2: Create 'All Skills' column per job\n",
    "    esco_taxonomy_df[\"All Skills\"] = (\n",
    "        esco_taxonomy_df[\"Essential Skill\"].fillna(\"\").astype(str) + \", \" +\n",
    "        esco_taxonomy_df[\"Essential Knowledge\"].fillna(\"\").astype(str) + \", \" +\n",
    "        esco_taxonomy_df[\"Optional Skill\"].fillna(\"\").astype(str) + \", \" +\n",
    "        esco_taxonomy_df[\"Optional Knowledge\"].fillna(\"\").astype(str)\n",
    "    )\n",
    "\n",
    "    # Step 3: Group by Job and create a set of lowercase skill strings\n",
    "    job_skills_df = (\n",
    "        esco_taxonomy_df.groupby([\"Job Role\", \"Job Description\"])[\"All Skills\"]\n",
    "        .apply(lambda skills: set(s.strip().lower() for line in skills for s in line.split(\",\") if s.strip()))\n",
    "        .reset_index(name=\"All Skills Set\")\n",
    "    )\n",
    "\n",
    "    # Step 4: Count how many matched skills appear in each job\n",
    "    job_skills_df[\"Matched Skills Count\"] = job_skills_df[\"All Skills Set\"].apply(\n",
    "        lambda skills: len(matched_skills.intersection(skills))\n",
    "    )\n",
    "\n",
    "    # Step 5: Filter and sort\n",
    "    matched_jobs = job_skills_df[job_skills_df[\"Matched Skills Count\"] > 1].copy()\n",
    "    matched_jobs = matched_jobs.sort_values(by=\"Matched Skills Count\", ascending=False)\n",
    "\n",
    "    # Step 6: Display results\n",
    "    print(\"\\n📋 Matched Jobs (At Least 2 Skills Matched):\")\n",
    "    print(\"-\" * 58)\n",
    "    print(\"| {:<30} | {:<20} |\".format(\"Job Role\", \"Matched Skills Count\"))\n",
    "    print(\"-\" * 58)\n",
    "    for _, row in matched_jobs.iterrows():\n",
    "        print(\"| {:<30} | {:<20} |\".format(row[\"Job Role\"], row[\"Matched Skills Count\"]))\n",
    "    print(\"-\" * 58)\n",
    "\n",
    "    return matched_jobs[[\"Job Role\", \"Matched Skills Count\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cybersecurity_analyst_skills = [\"MySQL\", \"PostgreSQL\", \"Database backup and recovery\", \"Performance tuning\", \"SQL\", \"Stored procedures\", \"Data modelling\", \"Oracle\", \"ETL\", \"Indexing\"]\n",
    "\n",
    "match_jobs(cybersecurity_analyst_skills, threshold=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a two-step pipeline for matching user-provided skills to the ESCO skills taxonomy using sentence embeddings and cosine similarity. The prepare_esco_embeddings() function takes a raw set of ESCO skills, cleans it by removing empty strings and extra whitespace, and then uses the pre-trained Sentence-BERT model (all-MiniLM-L6-v2) to encode the cleaned skills into numerical embeddings. These embeddings represent the semantic meaning of each skill in vector space. The match_user_skills() function then preprocesses the user's input skills (e.g., lowercasing, stripping whitespace), encodes them using the same SBERT model, and compares them to the ESCO embeddings using cosine similarity. For each user skill, it identifies the top matching ESCO skill(s) and returns the results in a DataFrame showing the original user skill, the closest matched ESCO skill, and their similarity score. This approach enables semantic matching even when the exact phrasing of skills differs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 user_skill          matched_esco_skill  cosine_similarity\n",
      "0                     mysql                       mysql              1.000\n",
      "1                postgresql                  postgresql              1.000\n",
      "2  database backup recovery              perform backup              0.637\n",
      "3          performance tune  monitor system performance              0.506\n",
      "4                       sql                  sql server              0.736\n",
      "5           store procedure                 objectstore              0.521\n",
      "6                data model                  data model              1.000\n",
      "7                    oracle  oracle relational database              0.753\n",
      "8                       etl                         apl              0.561\n",
      "9                     index                   algorithm              0.368\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_esco_embeddings(raw_skill_set):\n",
    "    \"\"\"\n",
    "    Cleans and encodes a set of ESCO skills using SBERT.\n",
    "    Returns the cleaned skill list, model, and embeddings.\n",
    "    \"\"\"\n",
    "    # Clean and convert to list\n",
    "    cleaned_skills = [s.strip() for s in raw_skill_set if s.strip() != '']\n",
    "    \n",
    "    # Load model once\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Encode ESCO skills\n",
    "    skill_embeddings = model.encode(cleaned_skills)\n",
    "    \n",
    "    return cleaned_skills, model, skill_embeddings\n",
    "\n",
    "\n",
    "def match_user_skills(user_skills, esco_skills, esco_embeddings, model, top_k=1):\n",
    "    \"\"\"\n",
    "    Matches user-entered skills to ESCO skills using cosine similarity.\n",
    "    Returns a DataFrame of best matches.\n",
    "    \"\"\"\n",
    "    user_skills = [preprocess_text(skill) for skill in user_skills]\n",
    "\n",
    "    user_embeddings = model.encode(user_skills)\n",
    "    \n",
    "    matches = []\n",
    "    for i, user_emb in enumerate(user_embeddings):\n",
    "        sims = cosine_similarity([user_emb], esco_embeddings)[0]\n",
    "        \n",
    "        top_indices = sims.argsort()[::-1][:top_k]  # get top_k matches\n",
    "        for idx in top_indices:\n",
    "            matches.append({\n",
    "                \"user_skill\": user_skills[i],\n",
    "                \"matched_esco_skill\": esco_skills[idx],\n",
    "                \"cosine_similarity\": round(sims[idx], 3)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "esco_skills, model, esco_embeddings = prepare_esco_embeddings(all_skills)\n",
    "\n",
    "cybersecurity_analyst_skills = [\"MySQL\", \"PostgreSQL\", \"Database backup and recovery\", \"Performance tuning\", \"SQL\", \"Stored procedures\", \"Data modelling\", \"Oracle\", \"ETL\", \"Indexing\"]\n",
    "df_matches = match_user_skills(cybersecurity_analyst_skills, esco_skills, esco_embeddings, model)\n",
    "\n",
    "print(df_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To DO:\n",
    "\n",
    "# have list of available skills and knowledge, allow user to select these pre defined skills/knowledge and show filtered jobs\n",
    "# check if it can match the skill in ESCO or tech tools rather than individually >= 90 and then return the ESCO skill. \n",
    "#check both and see which has the higher match i suppose? \n",
    "\n",
    "\n",
    "1. User enters their skills + knowledge and matches relevant jobs\n",
    "2. User selects their preferred job, enters their skills + knowledge and returns what they are missing\n",
    "3. User enters their CV and a list of jobs relevant to their CV is returned\n",
    "4. User selects their preferred job, enters their CV and application tells them what is missing and what has been found. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
